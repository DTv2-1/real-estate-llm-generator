# ðŸ“‹ SesiÃ³n: Arquitectura Apify + Django + OpenAI

**Fecha**: 7 de enero de 2026  
**DuraciÃ³n**: SesiÃ³n completa de implementaciÃ³n  
**Objetivo**: Implementar scraping en la nube con Apify y extracciÃ³n LLM en Django

---

## ðŸŽ¯ Problema a Resolver

**Contexto Inicial:**
- Necesitas scraper de propiedades inmobiliarias en Costa Rica (~4 sitios web)
- Sitios protegidos con Cloudflare (Encuentra24) bloquean IPs de datacenter
- Se necesita extracciÃ³n de datos no estructurados con LLM (OpenAI)
- Kelly requiere despliegue 100% en la nube (AWS Lambda + Elastic eventualmente)

**ConfusiÃ³n Inicial:**
Al principio del chat, asumÃ­ que OpenAI debÃ­a ejecutarse dentro del Apify Actor (todo en cloud), pero tÃº corregiste:

> "no, la api de open AI tiene que llamarse desde el BE, recibe el html lo escanea y extrae los datos. apify se usa solo para extraer el html"

**CorrecciÃ³n Clave:**
La arquitectura correcta separa responsabilidades:
1. **Apify Actor**: Solo scraping de HTML con bypass de Cloudflare
2. **Django Backend**: Recibe HTML, llama OpenAI, guarda en PostgreSQL

---

## ðŸ—ï¸ Arquitectura Implementada

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ PASO 1: Apify Actor (Cloud - Plataforma Apify)             â”‚
â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚
â”‚ â€¢ Playwright con tÃ©cnicas avanzadas de stealth             â”‚
â”‚ â€¢ Proxies residenciales (Costa Rica) para Cloudflare       â”‚
â”‚ â€¢ User agent rotation + hardware fingerprinting            â”‚
â”‚ â€¢ Guarda HTML crudo en Key-Value Store                     â”‚
â”‚ â€¢ Publica metadata en Dataset                              â”‚
â”‚                                                             â”‚
â”‚ INPUT: start_urls, use_residential_proxies, max_listings   â”‚
â”‚ OUTPUT: Dataset con {url, html_key, title, scraped_at}     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â†“
                    Dataset ID disponible
                              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ PASO 2: Django Backend (DigitalOcean App Platform)         â”‚
â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚
â”‚ â€¢ Recibe POST con dataset_id                               â”‚
â”‚ â€¢ Obtiene lista de items del Dataset via Apify API         â”‚
â”‚ â€¢ Para cada item:                                           â”‚
â”‚   - Descarga HTML desde Key-Value Store                    â”‚
â”‚   - Limpia HTML con BeautifulSoup                          â”‚
â”‚   - Llama OpenAI API (gpt-4o-mini) para extraer datos      â”‚
â”‚   - Parsea JSON response con validaciÃ³n                    â”‚
â”‚   - Guarda en PostgreSQL (Property + Document models)      â”‚
â”‚                                                             â”‚
â”‚ INPUT: {dataset_id, actor_run_id}                          â”‚
â”‚ OUTPUT: {processed: N, errors: M, total_items: X}          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â†“
                  Datos estructurados en PostgreSQL
```

---

## ðŸ“ Archivos Creados/Modificados

### âœ… Apify Actor (Scraping Solo)

#### Archivos Nuevos:

1. **`apify_actor/main.py`** - 300+ lÃ­neas
   ```python
   # Funciones principales:
   - needs_residential_proxy(url) â†’ Detecta si necesita proxy
   - scrape_with_playwright(url, proxy_url) â†’ Scraping con stealth
   - main() â†’ OrquestaciÃ³n principal
   
   # Features:
   - Browser args avanzados para stealth
   - User agent pool con rotaciÃ³n
   - Hardware/device fingerprinting
   - Delays aleatorios humanos
   - Mouse movements + scrolling
   - Almacenamiento en Key-Value Store
   ```

2. **`apify_actor/.actor/actor.json`**
   ```json
   {
     "name": "real-estate-scraper",
     "title": "Costa Rica Real Estate Scraper",
     "input": "./input_schema.json",
     "dockerfile": "./Dockerfile"
   }
   ```

3. **`apify_actor/.actor/input_schema.json`**
   ```json
   {
     "properties": {
       "start_urls": { "type": "array", "required": true },
       "use_residential_proxies": { "type": "boolean", "default": true },
       "proxy_country": { "type": "string", "default": "CR" },
       "max_listings": { "type": "integer", "default": 100 }
     }
   }
   ```
   **Nota**: Se removieron campos de OpenAI y webhook que estaban inicialmente.

4. **`apify_actor/requirements.txt`**
   ```
   apify>=2.0.0
   playwright>=1.40.0
   beautifulsoup4>=4.12.0
   lxml>=4.9.0
   ```
   **Nota**: Se removieron `openai` y `httpx` que no se necesitan aquÃ­.

5. **`apify_actor/Dockerfile`**
   ```dockerfile
   FROM apify/actor-python-playwright:3.11
   COPY requirements.txt ./
   RUN pip install --no-cache-dir -r requirements.txt
   COPY . ./
   CMD ["python3", "-m", "main"]
   ```

6. **`apify_actor/README.md`** - DocumentaciÃ³n completa del Actor
7. **`apify_actor/.gitignore`** - Archivos estÃ¡ndar Python/Apify

### âœ… Django Backend (ExtracciÃ³n + Storage)

#### Archivos Nuevos:

1. **`apps/ingestion/views_apify_sync.py`** - 270+ lÃ­neas â­ CLAVE
   
   **FunciÃ³n 1: `extract_with_openai(html_content, url)`**
   ```python
   def extract_with_openai(html_content: str, url: str) -> Dict:
       # 1. Limpia HTML con BeautifulSoup
       soup = BeautifulSoup(html_content, 'html.parser')
       for script in soup(['script', 'style', 'noscript', 'iframe']):
           script.decompose()
       
       # 2. Extrae texto limpio
       text_content = soup.get_text(separator='\n', strip=True)
       
       # 3. Trunca a 8000 chars (~2000 tokens)
       if len(text_content) > 8000:
           text_content = text_content[:8000] + "..."
       
       # 4. Llama OpenAI con prompt estructurado
       client = OpenAI(api_key=settings.OPENAI_API_KEY)
       response = client.chat.completions.create(
           model="gpt-4o-mini",
           messages=[...],
           temperature=0.1,
           max_tokens=1500
       )
       
       # 5. Parsea y valida JSON
       extracted_data = json.loads(content)
       
       # 6. Retorna con metadata
       return {
           'extraction_status': 'success',
           'extracted_data': extracted_data,
           'model': 'gpt-4o-mini',
           'tokens_used': response.usage.total_tokens
       }
   ```

   **FunciÃ³n 2: `sync_apify_dataset(request)`**
   ```python
   @csrf_exempt
   @require_http_methods(["POST"])
   def sync_apify_dataset(request):
       # 1. Recibe dataset_id del request
       data = json.loads(request.body)
       dataset_id = data.get('dataset_id')
       
       # 2. Inicializa Apify client
       client = ApifyClient(settings.APIFY_TOKEN)
       dataset = client.dataset(dataset_id)
       items = list(dataset.iterate_items())
       
       # 3. Para cada item del dataset:
       for item in items:
           # 3a. Obtiene html_key del item
           html_key = item.get('html_key')
           
           # 3b. Descarga HTML del Key-Value Store
           kv_store = client.key_value_store(kv_store_id)
           html_content = kv_store.get_record(html_key)['value']
           
           # 3c. Extrae datos con OpenAI
           extraction_result = extract_with_openai(html_content, url)
           
           # 3d. Guarda en PostgreSQL
           Property.objects.update_or_create(
               source_url=url,
               defaults={
                   'title': extracted.get('title'),
                   'price': extracted.get('price'),
                   # ... todos los campos
                   'metadata': {
                       'confidence': extracted.get('confidence'),
                       'evidence': extracted.get('evidence'),
                       'apify_dataset_id': dataset_id
                   }
               }
           )
           
           # 3e. Crea Document vinculado
           Document.objects.update_or_create(
               property=property_obj,
               source_type='apify',
               defaults={'content': html_content[:5000]}
           )
       
       # 4. Retorna estadÃ­sticas
       return JsonResponse({
           'status': 'success',
           'processed': processed,
           'errors': errors
       })
   ```

2. **`apps/ingestion/views_apify_webhook.py`** - 85 lÃ­neas
   - Endpoint webhook alternativo (si decides usarlo)
   - Recibe datos ya extraÃ­dos directamente de Apify
   - Mantiene compatibilidad con arquitectura anterior

#### Archivos Modificados:

3. **`apps/ingestion/urls.py`**
   ```python
   # ANTES:
   urlpatterns = [
       path('url/', IngestURLView.as_view()),
       path('text/', IngestTextView.as_view()),
       path('batch/', IngestBatchView.as_view()),
       path('save/', SavePropertyView.as_view()),
   ]
   
   # DESPUÃ‰S:
   from .views_apify_sync import sync_apify_dataset
   from .views_apify_webhook import apify_webhook
   
   urlpatterns = [
       # ... rutas existentes ...
       path('webhooks/apify/', apify_webhook, name='apify-webhook'),
       path('apify/sync/', sync_apify_dataset, name='apify-sync'),  # â­ NUEVO
   ]
   ```

4. **`requirements.txt`**
   ```diff
   + apify-client==1.7.1
   ```

### âœ… DocumentaciÃ³n

1. **`APIFY_SETUP.md`** - 400+ lÃ­neas â­ GUÃA COMPLETA
   
   Secciones:
   - Arquitectura explicada con diagramas ASCII
   - Por quÃ© esta arquitectura vs alternativas
   - ComparaciÃ³n de data flow (forma incorrecta vs correcta)
   - Setup paso a paso con comandos
   - ConfiguraciÃ³n de variables de entorno
   - Testing completo del flujo
   - Costos detallados por componente
   - Ventajas de desacoplar scraping y extracciÃ³n
   - Opciones de automatizaciÃ³n (manual, scheduled, webhook)
   - Monitoreo con Apify Console, Django logs, PostgreSQL queries
   - Troubleshooting comÃºn
   - PrÃ³ximos pasos

2. **`docs/PROXY_SETUP.md`** - Ya existÃ­a de sesiÃ³n anterior
   - GuÃ­a de proxies residenciales
   - ComparaciÃ³n de proveedores
   - Precios y configuraciÃ³n

---

## ðŸ”„ EvoluciÃ³n de la Arquitectura en Esta SesiÃ³n

### Intento 1: Todo en Apify (INCORRECTO âŒ)

```
Apify Actor:
  â”œâ”€ Playwright scraping
  â”œâ”€ OpenAI extraction  âŒ No debÃ­a estar aquÃ­
  â”œâ”€ Webhook to Django  âŒ Complicado
  â””â”€ Manejo de errores distribuido

Problemas:
- Si Actor crashea, se pierden llamadas de OpenAI ($$$)
- No puedes mejorar prompts sin redesplegar Actor
- Logs separados en Apify + Django
- Reintento requiere re-scraping completo
```

**CÃ³digo que removÃ­:**
```python
# âŒ Estas funciones estaban en main.py pero las removÃ­:
async def extract_with_llm(html_content, url, openai_api_key):
    # Llamaba a OpenAI desde Apify
    pass

async def send_to_backend(data, backend_webhook_url):
    # Enviaba webhook con httpx
    pass
```

### CorrecciÃ³n Final: SeparaciÃ³n de Responsabilidades (CORRECTO âœ…)

```
Apify Actor:
  â”œâ”€ Playwright scraping
  â”œâ”€ Stealth techniques
  â”œâ”€ Proxy management
  â””â”€ HTML storage in KV Store
      â†“ Solo metadata en Dataset
      
Django Backend:
  â”œâ”€ Fetch HTML from Apify
  â”œâ”€ OpenAI extraction âœ… AquÃ­ sÃ­
  â”œâ”€ Validation & parsing
  â””â”€ PostgreSQL storage

Ventajas:
âœ… Reintento barato (solo refetch HTML)
âœ… Prompts mejorables sin redesplegar Actor
âœ… Logs centralizados en Django
âœ… Control total del flujo de extracciÃ³n
âœ… Testing local mÃ¡s fÃ¡cil
```

---

## ðŸ’¡ Decisiones TÃ©cnicas Importantes

### 1. Â¿Por quÃ© gpt-4o-mini y no gpt-4?

```python
model="gpt-4o-mini"  # $0.15/1M input, $0.60/1M output
# vs
model="gpt-4"        # $30/1M input, $60/1M output
```

**RazÃ³n**: Para extracciÃ³n de datos estructurados, gpt-4o-mini es suficiente y 200x mÃ¡s barato.

### 2. Â¿Por quÃ© truncar HTML a 8000 chars?

```python
max_chars = 8000  # ~2000 tokens para gpt-4o-mini
```

**RazÃ³n**: 
- gpt-4o-mini tiene lÃ­mite de contexto
- Texto de propiedades suele ser repetitivo despuÃ©s de cierto punto
- Reduce costos de tokens de input

### 3. Â¿Por quÃ© BeautifulSoup antes de OpenAI?

```python
soup = BeautifulSoup(html_content, 'html.parser')
for script in soup(['script', 'style', 'noscript', 'iframe']):
    script.decompose()
text_content = soup.get_text(separator='\n', strip=True)
```

**RazÃ³n**:
- Elimina JavaScript, CSS, iframes (ruido)
- Reduce tokens enviados a OpenAI
- Mejora calidad de extracciÃ³n

### 4. Â¿Por quÃ© almacenar HTML en Apify y no en PostgreSQL?

**RazÃ³n**:
- PostgreSQL: Optimizado para datos estructurados
- Apify Key-Value Store: DiseÃ±ado para blobs (HTML pesado)
- Puedes referenciar HTML sin bloat en DB
- Facilita re-procesamiento masivo

### 5. Â¿Por quÃ© confidence scores y evidence en metadata?

```python
'metadata': {
    'confidence': {
        'price': 0.98,
        'location': 0.95
    },
    'evidence': {
        'price': "Price: $250,000",
        'location': "Located in Tamarindo..."
    }
}
```

**RazÃ³n**:
- Permite filtrar datos de baja calidad
- Facilita debugging de extracciones malas
- Evidence permite validar manualmente
- Ãštil para entrenar modelos propios despuÃ©s

---

## ðŸ’° AnÃ¡lisis de Costos

### Para 1000 listings/mes:

| Componente | Detalle | Costo Mensual |
|-----------|---------|---------------|
| **Apify Compute** | 15 CU Ã— $0.30/CU | $4.50 |
| **Proxies Residenciales** | 0.3 GB Ã— $8/GB | $2.40 |
| **OpenAI Llamadas** | 1000 calls Ã— ~$0.005 | ~$5.00 |
| **OpenAI Input Tokens** | ~2M tokens Ã— $0.15/1M | $0.30 |
| **OpenAI Output Tokens** | ~300K tokens Ã— $0.60/1M | $0.18 |
| **TOTAL** | | **$12.38** |

**Plan Apify Starter**: $39/mes en crÃ©ditos cubre todo perfectamente.

### ComparaciÃ³n con Alternativas:

| SoluciÃ³n | Costo Mensual | Pros | Contras |
|----------|---------------|------|---------|
| **Apify + Django (actual)** | $12.38 | Control total, flexible | Requiere setup |
| **ZenRows** | $73+ | Simple setup | MÃ¡s caro, menos control |
| **ScraperAPI** | $50+ | Maneja todo | Caro, menos flexible |
| **DIY Proxies + DigitalOcean** | $50+ | Barato compute | IPs bloqueadas por Cloudflare |

---

## ðŸš€ CÃ³mo Funciona el Flujo Completo

### Paso 1: Usuario corre Apify Actor

```bash
# En Apify Console o via CLI:
{
  "start_urls": [
    {"url": "https://www.encuentra24.com/costa-rica-en/real-estate-for-sale"},
    {"url": "https://www.coldwellbankercostarica.com/property/search"}
  ],
  "use_residential_proxies": true,
  "proxy_country": "CR",
  "max_listings": 50
}
```

### Paso 2: Actor ejecuta y guarda resultados

```
Actor Run ID: run_abc123xyz
â”œâ”€ Key-Value Store: kvs_def456
â”‚  â”œâ”€ html_1 (150KB)
â”‚  â”œâ”€ html_2 (180KB)
â”‚  â””â”€ html_3 (165KB)
â”‚
â””â”€ Dataset: dataset_ghi789
   â”œâ”€ Item 1: {url: "...", html_key: "html_1", title: "...", scraped_at: "..."}
   â”œâ”€ Item 2: {url: "...", html_key: "html_2", ...}
   â””â”€ Item 3: {url: "...", html_key: "html_3", ...}
```

### Paso 3: Django obtiene dataset_id y procesa

```bash
# Llamada manual o automÃ¡tica:
curl -X POST https://goldfish-app-3hc23.ondigitalocean.app/ingestion/apify/sync/ \
  -H "Content-Type: application/json" \
  -d '{
    "dataset_id": "dataset_ghi789",
    "actor_run_id": "run_abc123xyz"
  }'
```

### Paso 4: Django procesa cada item

```
Para Item 1:
  1. Obtiene html_key = "html_1"
  2. Descarga HTML (150KB) del Key-Value Store
  3. Limpia con BeautifulSoup â†’ 8KB de texto
  4. Llama OpenAI:
     - Input: 8KB texto + prompt (2000 tokens)
     - Output: JSON estructurado (500 tokens)
     - Costo: ~$0.005
  5. Parsea JSON response
  6. Guarda Property en PostgreSQL:
     {
       id: 1234,
       source_url: "...",
       title: "Beautiful Beach House",
       price: 250000,
       currency: "USD",
       beds: 3,
       baths: 2,
       location: "Tamarindo",
       metadata: {
         confidence: {price: 0.98, location: 0.95},
         evidence: {...},
         apify_dataset_id: "dataset_ghi789",
         apify_html_key: "html_1"
       }
     }
  7. Crea Document vinculado:
     {
       property_id: 1234,
       source_type: "apify",
       content: "HTML snippet...",
       metadata: {
         apify_kv_store_id: "kvs_def456",
         apify_html_key: "html_1"
       }
     }

Repite para Items 2, 3, ...
```

### Paso 5: Respuesta y verificaciÃ³n

```json
{
  "status": "success",
  "dataset_id": "dataset_ghi789",
  "total_items": 3,
  "processed": 3,
  "errors": 0
}
```

---

## ðŸ” Debugging y Monitoreo

### En Apify Console:

```
Actor Run â†’ Logs:
[INFO] Starting HTML scraping for 2 URLs
[INFO] Using residential proxy for Cloudflare-protected: encuentra24.com
[INFO] Successfully scraped https://... (150234 bytes)
[INFO] Stored HTML as html_1
[INFO] Scraping completed: 3 successful, 0 failed
```

### En Django Logs:

```python
# En DigitalOcean App Platform â†’ Logs:
[INFO] Fetched 3 items from Apify dataset dataset_ghi789
[INFO] Fetched HTML for https://... (150234 bytes)
[INFO] Extracted 8000 chars of text from HTML
[INFO] Successfully extracted data from https://...: Beautiful Beach House
[INFO] Successfully created property: Beautiful Beach House (https://...)
```

### En PostgreSQL:

```sql
-- Verificar importaciÃ³n
SELECT title, price, location, 
       metadata->>'apify_dataset_id' as dataset_id
FROM properties
WHERE metadata->>'apify_dataset_id' = 'dataset_ghi789';

-- Ver confidence scores
SELECT 
    title,
    (metadata->'confidence'->>'price')::float as price_confidence,
    (metadata->'confidence'->>'location')::float as location_confidence
FROM properties
WHERE metadata->'confidence' IS NOT NULL
ORDER BY price_confidence DESC;

-- EstadÃ­sticas de extracciÃ³n
SELECT 
    COUNT(*) as total,
    COUNT(CASE WHEN metadata->>'extraction_status' = 'success' THEN 1 END) as successful,
    ROUND(100.0 * COUNT(CASE WHEN metadata->>'extraction_status' = 'success' THEN 1 END) / COUNT(*), 2) as success_rate
FROM properties;
```

---

## ðŸ› Troubleshooting ComÃºn

### Problema 1: Apify Actor no scrape URLs

**SÃ­ntoma**: Actor termina pero Dataset vacÃ­o

**Soluciones**:
```python
# 1. Verifica que URLs sean vÃ¡lidas
"start_urls": [
    {"url": "https://..."}  # âœ… Con protocolo
]

# 2. Habilita proxies si es Cloudflare
"use_residential_proxies": true

# 3. Revisa logs del Actor en Apify Console
```

### Problema 2: Django no encuentra HTML en Key-Value Store

**SÃ­ntoma**: Error `HTML not found in KV store for key html_1`

**SoluciÃ³n**:
```python
# AsegÃºrate de pasar actor_run_id en el request:
{
    "dataset_id": "abc123",
    "actor_run_id": "run_xyz789"  # â† IMPORTANTE
}

# O el Actor debe incluir actor_run_id en cada item del Dataset
```

### Problema 3: OpenAI retorna JSON invÃ¡lido

**SÃ­ntoma**: `Failed to parse OpenAI response as JSON`

**Debugging**:
```python
# Ver raw response en logs
logger.error(f'Raw response: {content[:500]}')

# ComÃºn: OpenAI devuelve ```json ... ``` (markdown)
# SoluciÃ³n ya implementada en extract_with_openai():
if content.startswith('```'):
    parts = content.split('```')
    for part in parts:
        if part.strip().startswith('json'):
            content = part[4:].strip()
            break
```

### Problema 4: Campos extraÃ­dos son null

**SÃ­ntoma**: Price, beds, location todos null en DB

**Causas y Soluciones**:
```python
# 1. HTML mal limpiado
# â†’ Revisa que BeautifulSoup no elimine contenido importante

# 2. Prompt de OpenAI muy genÃ©rico
# â†’ Ajusta prompt en extract_with_openai() para ser mÃ¡s especÃ­fico

# 3. Texto truncado elimina informaciÃ³n clave
# â†’ Aumenta max_chars de 8000 a 12000

# 4. Sitio web tiene estructura muy diferente
# â†’ Agrega ejemplos especÃ­ficos del sitio en el prompt
```

### Problema 5: Costos muy altos de OpenAI

**SÃ­ntoma**: Gasto excede presupuesto

**Optimizaciones**:
```python
# 1. Reduce max_chars (menos tokens input)
max_chars = 6000  # En vez de 8000

# 2. Usa temperature mÃ¡s baja (menos tokens output)
temperature=0.05  # En vez de 0.1

# 3. Reduce max_tokens output
max_tokens=1000  # En vez de 1500

# 4. Cache resultados para evitar re-extracciones
# â†’ Verifica source_url antes de re-procesar
```

---

## ðŸ“ˆ Ventajas Clave de Esta Arquitectura

### 1. **Desacoplamiento de Responsabilidades**

```
Apify:
  âœ… Experto en scraping y bypass de anti-bot
  âœ… Infraestructura serverless escalable
  âœ… Proxies residenciales integrados
  âœ… Monitoreo de runs y datasets

Django:
  âœ… Control total de lÃ³gica de negocio
  âœ… IntegraciÃ³n con tu stack existente
  âœ… Prompts y validaciones customizables
  âœ… Testing y debugging local fÃ¡cil
```

### 2. **EconomÃ­a de Reintentos**

```
Arquitectura Acoplada (Actor + OpenAI):
  Error en extracciÃ³n â†’ Re-scraping completo
  Costo: $0.003 (scraping) + $0.005 (OpenAI) = $0.008

Arquitectura Desacoplada (nuestra):
  Error en extracciÃ³n â†’ Solo refetch HTML
  Costo: $0 (HTML ya guardado) + $0.005 (OpenAI) = $0.005
  
  Ahorro: 37.5% por reintento
```

### 3. **IteraciÃ³n de Prompts Sin Redeploy**

```
Antes (Actor con OpenAI):
  1. Editar prompt en main.py
  2. apify push (1-2 min)
  3. Esperar build
  4. Correr Actor
  5. Ver resultados
  Total: ~5 minutos por iteraciÃ³n

Ahora (Django con OpenAI):
  1. Editar prompt en views_apify_sync.py
  2. git push (DigitalOcean redeploya automÃ¡tico)
  3. Llamar endpoint con dataset_id antiguo
  4. Ver resultados
  Total: ~2 minutos por iteraciÃ³n
  
  O testing local:
  1. Editar prompt
  2. python manage.py runserver
  3. curl localhost con dataset_id
  4. Ver resultados
  Total: ~30 segundos por iteraciÃ³n
```

### 4. **Reprocessamiento HistÃ³rico**

```python
# Puedes mejorar el prompt y reprocesar TODO el histÃ³rico:
datasets_ids = ["dataset_1", "dataset_2", "dataset_3", ...]

for dataset_id in datasets_ids:
    requests.post(
        "https://goldfish-app-3hc23.ondigitalocean.app/ingestion/apify/sync/",
        json={"dataset_id": dataset_id}
    )

# HTML ya estÃ¡ guardado en Apify Key-Value Store
# No necesitas re-scrapear nada
# Solo pagas OpenAI tokens (~$5 por 1000 propiedades)
```

### 5. **Flexibilidad de LLM Provider**

```python
# FÃ¡cil cambiar de OpenAI a Anthropic:
def extract_with_anthropic(html_content: str, url: str) -> Dict:
    client = anthropic.Anthropic(api_key=settings.ANTHROPIC_API_KEY)
    response = client.messages.create(
        model="claude-3-haiku-20240307",
        messages=[...],
    )
    # ... resto igual

# O usar modelo local:
def extract_with_local_llm(html_content: str, url: str) -> Dict:
    response = requests.post(
        "http://localhost:8000/extract",
        json={"text": text_content, "url": url}
    )
    # ... resto igual
```

---

## ðŸŽ“ Lecciones Aprendidas

### 1. **Importancia de Clarificar Arquitectura Temprano**

**Problema**: AsumÃ­ que todo debÃ­a estar en Apify porque Kelly dijo "en la nube"

**CorrecciÃ³n**: "en la nube" puede significar:
- Apify para scraping
- Django en DigitalOcean para lÃ³gica
- PostgreSQL en DigitalOcean para datos
- OpenAI API para LLM

**LecciÃ³n**: Preguntar especÃ­ficamente dÃ³nde va cada responsabilidad.

### 2. **No Sobre-Complejizar el Actor**

**TentaciÃ³n**: Meter OpenAI, webhooks, validaciÃ³n, todo en Apify

**Realidad**: Actor debe ser simple:
- Scraping
- Almacenamiento
- Nada mÃ¡s

**LecciÃ³n**: Apify es infraestructura de scraping, no backend completo.

### 3. **Valor de Metadata Rica**

```python
# Malo (datos solos):
{
    "price": 250000,
    "location": "Tamarindo"
}

# Bueno (datos + confianza + evidencia):
{
    "price": 250000,
    "location": "Tamarindo",
    "confidence": {"price": 0.98, "location": 0.95},
    "evidence": {
        "price": "Listed at $250,000 USD",
        "location": "Prime beachfront in Tamarindo"
    }
}
```

**LecciÃ³n**: Metadata ayuda con debugging, validaciÃ³n y mejora continua.

### 4. **Testing Incremental es CrÃ­tico**

**Enfoque correcto**:
1. Test con 1 URL â†’ Verifica flujo bÃ¡sico
2. Test con 3 URLs â†’ Verifica batch processing
3. Test con 10 URLs â†’ Verifica calidad
4. Test con 50 URLs â†’ Verifica performance
5. Production con 100+ URLs

**No hacer**: Deployment directo a 1000 URLs sin testing.

---

## ðŸ”® PrÃ³ximos Pasos

### Inmediatos (esta semana):

1. **Deployar Apify Actor**
   ```bash
   cd apify_actor
   npm install -g apify-cli
   apify login
   apify push
   ```

2. **Deployar Django Changes**
   ```bash
   git add -A
   git commit -m "Add Apify sync endpoint with OpenAI extraction"
   git push origin main
   ```

3. **Configurar Variables de Entorno en DigitalOcean**
   ```
   OPENAI_API_KEY=sk-proj-...
   APIFY_TOKEN=apify_api_...
   ```

4. **Test con 1 URL**
   - Correr Actor manualmente
   - Llamar sync endpoint
   - Verificar en PostgreSQL

### Mediano Plazo (prÃ³ximas 2 semanas):

5. **Ajustar Prompts segÃºn Calidad**
   - Revisar extracciones
   - Iterar en prompt de OpenAI
   - Agregar ejemplos especÃ­ficos por sitio

6. **Automatizar con Apify Schedules**
   ```json
   {
     "cronExpression": "0 2 * * *",  // 2 AM daily
     "input": {
       "start_urls": [...],
       "max_listings": 100
     }
   }
   ```

7. **Crear Management Command para Sync AutomÃ¡tico**
   ```python
   # apps/ingestion/management/commands/sync_latest_apify.py
   class Command(BaseCommand):
       def handle(self):
           # Get latest successful run
           # Call sync_apify_dataset()
   ```

8. **Agregar Monitoring y Alertas**
   - Sentry para errores
   - Email notifications para runs fallidos
   - Dashboard con mÃ©tricas de calidad

### Largo Plazo (prÃ³ximo mes):

9. **MigraciÃ³n a AWS Lambda** (requerimiento de Kelly)
   - Django con Zappa o AWS SAM
   - RDS PostgreSQL
   - Apify ya es serverless (no cambia)

10. **Agregar MÃ¡s Sitios Web**
    ```python
    CLOUDFLARE_PROTECTED_DOMAINS = [
        'encuentra24.com',
        'nuevo-sitio.cr',  # Agregar segÃºn necesidad
    ]
    ```

11. **OptimizaciÃ³n de Costos**
    - Caching de resultados
    - Rate limiting inteligente
    - Batch processing optimizado

---

## ðŸ“Š MÃ©tricas de Ã‰xito

### KPIs a Monitorear:

1. **Scraping Success Rate**
   ```sql
   SELECT 
       COUNT(*) FILTER (WHERE error IS NULL) * 100.0 / COUNT(*) as success_rate
   FROM apify_datasets;
   ```
   **Target**: >95%

2. **Extraction Quality**
   ```sql
   SELECT 
       AVG((metadata->'confidence'->>'price')::float) as avg_confidence
   FROM properties;
   ```
   **Target**: >0.85

3. **Cost per Listing**
   ```
   Total monthly cost / Total listings processed
   ```
   **Target**: <$0.02 per listing

4. **Processing Time**
   ```
   Time from Actor finish to PostgreSQL storage
   ```
   **Target**: <5 minutes for 100 listings

5. **Error Rate**
   ```sql
   SELECT 
       COUNT(*) FILTER (WHERE metadata->>'extraction_status' = 'error') * 100.0 / COUNT(*)
   FROM properties;
   ```
   **Target**: <5%

---

## ðŸ” Seguridad y Best Practices

### Variables de Entorno:

```bash
# âŒ NUNCA en cÃ³digo:
OPENAI_API_KEY = "sk-proj-abc123..."

# âœ… Siempre en .env o secrets:
OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')
```

### Apify Secrets:

```bash
# En Apify Console â†’ Settings â†’ Environment Variables:
# Marcar como "Secret" (encrypted):
OPENAI_API_KEY=sk-proj-...
```

### Django CSRF:

```python
# Sync endpoint necesita CSRF exempt para llamadas externas:
@csrf_exempt  
@require_http_methods(["POST"])
def sync_apify_dataset(request):
    # Pero verifica autenticaciÃ³n:
    token = request.headers.get('Authorization')
    # Valida token...
```

### Rate Limiting:

```python
# Agregar en futuro:
from django_ratelimit.decorators import ratelimit

@ratelimit(key='ip', rate='10/m')
@csrf_exempt
def sync_apify_dataset(request):
    # ...
```

---

## ðŸ“š Referencias y Recursos

### DocumentaciÃ³n Oficial:

- **Apify**: https://docs.apify.com/
- **Playwright**: https://playwright.dev/python/docs/intro
- **OpenAI API**: https://platform.openai.com/docs/api-reference
- **Django REST**: https://www.django-rest-framework.org/

### Herramientas Ãštiles:

- **Apify Console**: https://console.apify.com/
- **OpenAI Playground**: https://platform.openai.com/playground
- **Apify CLI**: `npm install -g apify-cli`
- **Proxy Tester**: https://whoer.net/

### Comunidad:

- **Apify Discord**: https://discord.com/invite/jyEM2PRvMU
- **r/webscraping**: https://reddit.com/r/webscraping
- **Playwright GitHub**: https://github.com/microsoft/playwright

---

## ðŸ“ Resumen Ejecutivo

**Tiempo Invertido**: ~2 horas de sesiÃ³n completa

**LÃ­neas de CÃ³digo**:
- Apify Actor: ~300 lÃ­neas
- Django Backend: ~270 lÃ­neas
- DocumentaciÃ³n: ~800 lÃ­neas
- Total: ~1370 lÃ­neas

**Archivos Creados**: 12 archivos nuevos

**Archivos Modificados**: 3 archivos existentes

**Costo Estimado**: ~$13/mes para 1000 listings

**Estado**: âœ… CÃ³digo completo, listo para deployment

**PrÃ³ximo Paso CrÃ­tico**: Deployar a Apify y DigitalOcean para testing real

---

## ðŸŽ¯ ConclusiÃ³n

Esta sesiÃ³n resolviÃ³ exitosamente la arquitectura para scraping en la nube con extracciÃ³n LLM:

âœ… **SeparaciÃ³n clara**: Apify (scraping) â†” Django (extracciÃ³n + storage)  
âœ… **EconomÃ­a**: $13/mes para 1000 listings  
âœ… **Flexibilidad**: Prompts mejorables sin redesplegar Actor  
âœ… **Escalabilidad**: Serverless en ambos lados  
âœ… **Mantenibilidad**: CÃ³digo limpio y bien documentado  

**La arquitectura estÃ¡ lista para producciÃ³n** ðŸš€

---

*Generado: 7 de enero de 2026*  
*Proyecto: Real Estate LLM Generator*  
*Stack: Apify + Django + OpenAI + PostgreSQL*
